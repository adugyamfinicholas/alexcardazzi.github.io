[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Alexander Cardazzi",
    "section": "",
    "text": "Assistant Professor | Old Dominion University | acardazz@odu.edu\n\n\n\n\n\n\nApplied microeconomist with research interests including transportation, urban, and sports economics.\n\n\nAssistant Professor of Economics at Old Dominion University\n\n\nPh.D. Economics - West Virginia University (2017-2021)\n\n\nB.S. Mathematics - Ramapo College (2013-2017)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Assistant Professor | Old Dominion University",
    "section": "",
    "text": "Twitter: Some Economics Talk, Mostly Basketball Talk\n\n\nCurrent University Email: Some Basketball Talk, Mostly Economics Talk\n\n\n\nCheck out my twitter threads about the NBA!\n\n\n\nHover over images below for a surprise…"
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Alexander Cardazzi",
    "section": "",
    "text": "Information shocks and celebrity exposure: The effect of “Magic” Johnson on AIDS diagnoses and mortality in the U.S.; Health Economics (2023)\n\n\n\n\n\nWith J. Martin & Z. Rodriguez\nAbstract: We present evidence that Earvin “Magic” Johnson’s announcement that he contracted HIV served as a public-health catalyst for rapidly correcting the public’s understanding of who was at risk of infection. Using a novel identification strategy, we present evidence that there was a large but temporary increase in the number of AIDS diagnoses for heterosexual men following the announcement. This effect was concentrated in areas with greater prior exposure to Johnson. We show that these men were both more likely to have been diagnosed via a formal blood test and less likely to die within one decade of their initial diagnosis – suggesting that Johnson’s announcement caused an intertemporal substitution in testing which prolonged patients’ lifespans as a result of earlier access to medical care. We estimate that Johnson’s announcement caused approximately 800 additional men to discover their underlying AIDS diagnosis and, of whom, were more likely to live at least one decade beyond their initial diagnosis date.\n\n\n\n\n\n\n\n\n\nDo sporting events amplify airborne virus transmission? Causal evidence from US professional team sports; Sports Economic Review (2023)\n\n\n\n\n\nWith B. Humphreys, J. Ruseski, B. Soebbing & N. Watanabe\nAbstract: The COVID-19 pandemic shut down sporting events worldwide. Local policy makers and league officials face important decisions about restarting play, especially in professional leagues that draw large numbers of spectators to games. We analyze the impact of professional sporting events on local seasonal influenza mortality to develop evidence that will help inform sports league reopening policy decisions. Results from a difference-in-differences model applied to data from a sample of US cities that gained new professional sports teams over the period 1962-2016 show that the presence of games in these cities increased local influenza mortality by between 4% and 24%, depending on sport, relative to cities with no professional sports teams and relative to mortality in those cities before a new team arrived. Influenza mortality fell in cities with teams in some years when work stoppages occurred in sports leagues. Sports league reopening policies should take into account the role played by sporting events in increasing local seasonal flu mortality.\n\n\n\n\n\n\n\n\n\nEconomic Freedom and One-Way Truck Rental Prices: An Empirical Note; The American Journal of Economics and Sociology (2023)\n\n\n\n\n\nWith R. Lawson\nThis study examines the one-way truck rental prices for 378 cities. There are large price differentials in one-way rental prices between city pairs. The pull of people toward higher economic freedom locales and push away from lower economic freedom locales is found to be an important determinant of the city-pair price differentials.\n\n\n\n\n\n\n\n\n\nMass Gathering Sport Events and The Spread of Viral Respiratory Infection: Japanese Professional Baseball and Influenza; Journal of Sports Economics (2022)\n\n\n\n\n\nWith H. Funahashi & N. Watanabe\nAbstract: Using weekly-level influenza case data from all 47 prefectures in Japan alongside data from Nippon Professional Baseball (NPB) league from 1999 to 2018, we examine the effect of hosting games on local influenza transmission. The results highlight that during the flu season, for every NPB game held at its home ballpark, there is an average increase of 0.18 cases per sentinel medical institution (SMI) between that week and the following week. The effects are robust to different specifications and placebo tests. This translates to about a 0.1% increase in the number of cases during the overlap of NPB and flu seasons.\n\n\n\n\n\n\n\n\n\nEmotional Cues and Violent Behavior: Unexpected Basketball Losses Increase Incidents of Family Violence; Journal of Law, Economics, & Organization (2022)\n\n\n\n\n\nWith B. Humphreys, B. McCannon & Z. Rodridguez\nAbstract: Domestic violence generates long-term effects on offenders, victims, and other household members. While coercive behavior explains some family violence, aggression can also be reactive, triggered by emotional stimulus. Insight into triggers of family violence can inform policy and mitigate abusive behavior. Card and Dahl (2011) undertook a novel analysis of family violence triggers using unexpected losses by American professional football teams. We extend research on this trigger using data from National Basketball Association (NBA) games. Our results show that unexpected NBA losses lead to increased in-home violence. Heterogeneity analyses show that these effects are larger for weekend games, when referees are fatigued, and closer to the playoff season.\n\n\n\n\n\n\n\n\n\nEmployee Satisfaction and Stock Returns During the COVID-19 Pandemic; Journal of Behavioral and Experimental Finance (2021)\n\n\n\n\n\nWith M. Becker & Z. McGurk\nAbstract: The COVID-19 Pandemic has had an unprecedented impact on how employees and employers operate. Employees, directly affected by workplace changes, may provide information regarding future efficiencies. As a result, crowdsourced employee satisfaction (\\(ES\\)) reviews mentioning the COVID-19 Pandemic may contain useful information regarding the future profitability of these firms. We utilize crowdsourced COVID-19 Pandemic specific \\(ES\\) obtained from Glassdoor.com to determine the impact on abnormal stock returns for public firms from March–December 2020. We find evidence that higher COVID-19 \\(ES\\) is related to higher abnormal stock returns. While non-COVID \\(ES\\) is found not to be related to abnormal stock returns.\n\n\n\n\n\n\n\n\n\nReadership and Citations as Alternative Measures of Impact; Constitutional Political Economy (2021)\n\n\n\n\n\nWith R. Congleton & A. Marsella\nAbstract: This paper undertakes a statistical analysis of citations and readership of papers published in the journal Constitutional Political Economy. Its focus is not the usual attempt to assess the relative impact of articles or authors but rather to suggest that readership (downloads) is a more general measure of impact and one that should be given more attention. Downloads are not simply a product of citations; nor are citations a simple product of downloads. They are distinct measures of impact. Papers and authors are evidently judged one at a time by their readers and by those who subsequently cite papers that they have read."
  },
  {
    "objectID": "teaching.html",
    "href": "teaching.html",
    "title": "Classroom Experience",
    "section": "",
    "text": "Classroom Experience\nOld Dominion University\n\nIntermediate Microeconomics - F ’22\nTime Series (MA, PhD) - F ’22\nTransportation Economics - S ’23, F ’23\nEconomics, Causality, and Analytics - F ’23\n\nWest Virginia University\n\nIntermediate Microeconomics - Summer ’19\nBusiness Statistics - F ’19, ’21; S ’20, ’22\nIntroductory Econometrics - F ’20, ’21 (MA)\n\n\n\nNon-Classroom Experience\nClaremont Graduate University - Two Day Programming Workshop (January ’19)\n\nIntroductory Web Scraping in R\n\nWest Virginia University - Empirical Workshop (July ’19 - July ’22)\n\nIntroduction to R Programming (’19 - ’21)\nRegression in R (’20, ’21)\nStrings, Loops, and If (’21)\nPlotting and Maps (’21, ’22)\nIntroductory (’20 - ’22) & Advanced (’19 - ’22) Web Crawling & Scraping in R"
  },
  {
    "objectID": "research.html#working-papers",
    "href": "research.html#working-papers",
    "title": "Alexander Cardazzi",
    "section": "Working Papers",
    "text": "Working Papers\n\n\n\n\n\n\nMessaging and Driving: An Analysis of Dynamic Message Signs in Virginia\n\n\n\n\n\nAbstract: Traffic fatalities accounted for 1.3% of all deaths in the United States in 2017 and the average American lost about 100 hours due to congestion in 2019. One tool transportation departments (DOTs) use to address these issues is Dynamic Message Signs (DMS). DMS convey traffic conditions and occasional safety reminders to drivers in order to increase attentiveness and reduce harmful behavior. This study leverages variation in the text and formatting of messages displayed by Virginia’s DMS to explain detailed speed and crash data near DMS. This study reports no significant differences in crash risk nor speed when DMS display safety messages compared to default messages. However, this study does uncover large and significant differences in crash risk and speed when DMS transition, or cycle, between multiple messages, although the effect only lasts for 3-5 kilometers. Results indicate that multi-page messages increased crashes by 1.5% in 2019, and reduced vehicle speed around DMS by 2-4%, relative to single page messages. Although DMS can provide valuable, actionable information to drivers, DOTs should be more selective in the timing and formatting of messages as to not impose additional externalities on drivers.\n\n\n\n\n\n\n\n\n\nCrowdsourced Employee Sentiment and Abnormal Stock Returns\n\n\n\n\n\nWith M. Becker & Z. McGurk\nAbstract: Previous literature has found crowdsourced employee sentiment obtained from Glassdoor.com is related to stock returns. Evidence has shown this data can suffer from some abnormalities which may limit its usefulness. To account for these discrepancies, we utilize textual analysis through the multinominal inverse regression method to create monthly firm specific expected employee sentiment indexes for US public firms from the period 2008-2019. We test the implications of the previous literature to determine if our expected employee sentiment index is related to the cross section of abnormal returns. We find evidence that our expected employee sentiment index is related to the cross section of abnormal returns. Further, we find that employee sentiment as estimated similar to the previous literature is no longer related to stock returns. These result are robust to different specifications.\n\n\n\n\n\n\n\n\n\nAlternate Definitions and Measures of Road Smoothness\n\n\n\n\n\nWith M. Bock\n\n\n\n\n\n\n\n\n\nTrust the Process: Examining Tanking in the NBA\n\n\n\n\n\nWith C. Johnson and E. Minuci\n\n\n\n\n\n\n\n\n\nErrors and Performance: Evidence from NBA Referee\n\n\n\n\n\nWith Z. Rodriguez"
  },
  {
    "objectID": "research.html#under-review",
    "href": "research.html#under-review",
    "title": "Alexander Cardazzi",
    "section": "Under Review",
    "text": "Under Review\n\n\n\n\n\n\nEffects of Pavement Smoothness on Traffic Outcomes: Evidence from California\n\n\n\n\n\nWith M. Bock & B. Humphreys; NBER Research Grant: Economics of Transportation in the 21st Century); Revisions Requested at Journal of Urban Economics\nAbstract: Road maintenance constitutes a significant component of public transportation spending at all levels of government. Formulation of efficient transportation infrastructure policy requires information about factors affecting road and traffic conditions. We generate the first causal evidence that decreasing pavement quality impacts vehicle crash rates and decreases average speed. Results from Instrumental Variable models using spatially and temporally disaggregated data from Federal-Aid Highway System (FAHS) roads in California show statistically and economically significant increases in vehicle crash rates and decreases in average vehicle speed caused by road damage. These impacts imply significant increases in social costs attributable to road damage.\n\n\n\n\n\n\n\n\n\nLasting Impacts of Stop & Frisk: Evidence From The Housing Market in New York City\n\n\n\n\n\nUnder review\nAbstract: Stop & Frisk, an aggressive policing tactic, was employed in New York City until a judge ruled it unconstitutional in 2013. This study uses variation in Stop & Frisk activity resulting from the ruling to estimate the tactic’s effect on property prices. Results indicate prices increased for properties most exposed to Stop & Frisk, relative to nearby properties, once the NYPD was ordered to cease Stop & Frisk activity. This increase was driven by white sellers, however, as values remained constant for non-white sellers. This is likely due to broken trust and divergent beliefs regarding future Stop & Frisk activity.\n\n\n\n\n\n\n\n\n\nDemand For Offense: Designated Hitters and MLB Attendance\n\n\n\n\n\nWith Z. Rodriguez; Under review\nAbstract: In 2022, Major League Baseball changed its rules to include the designated hitter position in both the American and National Leagues. Up until that year, designated hitters were only used by American League teams. This rule change creates an environment where half of a major sports league is given a shock to offensive production. We analyze the demand for offense by investigating how the designated hitter affects attendance and offensive production for National League teams. Our results contribute to a rich literature that analyzes how offense affects fan participation, as we provide the first evidence using causal inference that offense increases fan attendance. Using a difference-in-differences framework, we show that the offense created by a designated hitter increases attendance by about 3.5-7.9% at National League home games as a result of the rule change. Given a point estimate increase in total offense of about 4.85%, we find evidence that attendance is likely elastic with respect to total offense."
  },
  {
    "objectID": "basketball.html",
    "href": "basketball.html",
    "title": "Casual Basketball Artifacts",
    "section": "",
    "text": "No Need To Be Selfish in Your Career High Scoring Game\n\n\n\nLuka Doncic scored 46 points last night for a career high (CH). What's more impressive is he also had 12 assists. I would think a player would have to play a bit more selfishly in order to score a CH. Luka's 12 assists does not feel very selfish.\n\n— alex cardazzi (@ACardazzi) February 14, 2021\n\n\n\nThe Days Between Contracts\n\n\n\nNBA players sign contracts worth a lot of money relative to us mortals. As an outlier example, LeBron James was paid $35 million in 2019 by the Lakers. A random, current players on the front page of @bball_ref, PJ Tucker, made $8 million in 2019.\n\n— alex cardazzi (@ACardazzi) August 30, 2020\n\n\n\nAggregation Issue: basketball-reference.com\n\n\n\nSuper niche @bball_ref thread: I am looking at some game logs for players and finding some inconsistencies. For example, Alvan Adams is reported to have obtained 157 ORB and 389 DRB in the 1980-81 season. However, there was a single game where O/DRB were not recorded.\n\n— alex cardazzi (@ACardazzi) August 17, 2020\n\n\n\nAre NBA Players Lying About Their Weights?\n\n\n\nA lot of NBA players lie about their height. In the beginning of the 2019-20 NBA season, the NBA standardized how they measure player heights. Here are yearly height changes since the 1997-98 NBA season. This went from under 20 changes per year to about 250 in one season. pic.twitter.com/77QPajhclj\n\n— alex cardazzi (@ACardazzi) May 31, 2020\n\n\n\nThe Explosion of the Woj Bomb\n\n\n\nIs there a single Twitter notification more exciting than a Woj Bomb in early July? Perhaps you care more about the barrage in February, but July is when NBA free agency (usually) begins. As mere mortals, Woj is our best glimpse inside the offices of GMs across the NBA.\n\n— alex cardazzi (@ACardazzi) March 18, 2020\n\n\n\nOrdering Free Throws\n\n\n\nOn yesterday's episode of Buckets (w/ @BillyScafuri), @blumenfeld started with a cold open guessing some free throw probabilities. Being the nerd I am, I did the analysis.\n\n— alex cardazzi (@ACardazzi) March 6, 2020\n\n\n\nA (Jersey) Numbers Game\n\n\n\nRecently, there have been a lot of players who have changed their numbers in honor of Kobe Bryant. I was interested in the frequency at which players choose their numbers, so I went to basketball reference for the data.\n\n— alex cardazzi (@ACardazzi) February 1, 2020\n\n\n\nComparing Players’ Best Game & Best Season\n\n\n\nDuring @jakeandamir 's most recent Buckets podcast, Amir says an NBA player's best game should be 2x their best season scoring average. I gathered all NBA/ABA/BAA player's top scoring game, season, and their career average from basketball reference.\n\n— alex cardazzi (@ACardazzi) December 13, 2019\n\n\n\nIndistinguishable Points & Rebounds (Wilt vs Drummond)\n\n\n\nFor Wilt's last 7 seasons, you can play this cool game where someone says his rebounding and scoring averages for the year and someone else has to guess which is which. Specifically, in 1966-67, he had nearly identical averages of 24.2 and 24.1. pic.twitter.com/QvLFFE8HSN\n\n— alex cardazzi (@ACardazzi) December 1, 2019\n\n\n\nThe Rebounding Gravity of Russell Westbrook\n\n\n\nRussell Westbrook (RW) is one of the most polarizing players in the league today. Here's a plot (data from 2000-2020) showing the \"gravity\" he has on the glass. The interpretation, and implication, of this phenomenon is fairly interesting. pic.twitter.com/SfJQdaOuHU\n\n— alex cardazzi (@ACardazzi) November 9, 2019\n\n\n\nLeBron James Never Records His Average\n\n\n\nI've always been interested in how LBJ has averaged 27-7-7 (27.16-7.41-7.23) in the regular season for his career but never has had a game with that exact line. I looked at his games logs (excluding this season) and here's what I found.\n\n— alex cardazzi (@ACardazzi) November 7, 2019"
  },
  {
    "objectID": "research.html#publications",
    "href": "research.html#publications",
    "title": "Alexander Cardazzi",
    "section": "Publications",
    "text": "Publications\n\n\n\n\n\n\nReadership and Citations as Alternative Measures of Impact\n\n\n\n\n\nWith R. Congleton & A. Marsella; Constitutional Political Economy (2021)\nAbstract: This paper undertakes a statistical analysis of citations and readership of papers published in the journal Constitutional Political Economy. Its focus is not the usual attempt to assess the relative impact of articles or authors but rather to suggest that readership (downloads) is a more general measure of impact and one that should be given more attention. Downloads are not simply a product of citations; nor are citations a simple product of downloads. They are distinct measures of impact. Papers and authors are evidently judged one at a time by their readers and by those who subsequently cite papers that they have read.\n\n\n\n\n\n\n\n\n\nEmployee Satisfaction and Stock Returns During the COVID-19 Pandemic\n\n\n\n\n\nWith M. Becker & Z. McGurk; Journal of Behavioral and Experimental Finance (2021)\nAbstract: The COVID-19 Pandemic has had an unprecedented impact on how employees and employers operate. Employees, directly affected by workplace changes, may provide information regarding future efficiencies. As a result, crowdsourced employee satisfaction (\\(ES\\)) reviews mentioning the COVID-19 Pandemic may contain useful information regarding the future profitability of these firms. We utilize crowdsourced COVID-19 Pandemic specific \\(ES\\) obtained from Glassdoor.com to determine the impact on abnormal stock returns for public firms from March–December 2020. We find evidence that higher COVID-19 \\(ES\\) is related to higher abnormal stock returns. While non-COVID \\(ES\\) is found not to be related to abnormal stock returns.\n\n\n\n\n\n\n\n\n\nEmotional Cues and Violent Behavior: Unexpected Basketball Losses Increase Incidents of Family Violence\n\n\n\n\n\nWith B. Humphreys, B. McCannon & Z. Rodridguez; Journal of Law, Economics, & Organization (2022)\nAbstract: Domestic violence generates long-term effects on offenders, victims, and other household members. While coercive behavior explains some family violence, aggression can also be reactive, triggered by emotional stimulus. Insight into triggers of family violence can inform policy and mitigate abusive behavior. Card and Dahl (2011) undertook a novel analysis of family violence triggers using unexpected losses by American professional football teams. We extend research on this trigger using data from National Basketball Association (NBA) games. Our results show that unexpected NBA losses lead to increased in-home violence. Heterogeneity analyses show that these effects are larger for weekend games, when referees are fatigued, and closer to the playoff season.\n\n\n\n\n\n\n\n\n\nMass Gathering Sport Events and The Spread of Viral Respiratory Infection: Japanese Professional Baseball and Influenza\n\n\n\n\n\nWith H. Funahashi & N. Watanabe; Journal of Sports Economics (2022)\nAbstract: Using weekly-level influenza case data from all 47 prefectures in Japan alongside data from Nippon Professional Baseball (NPB) league from 1999 to 2018, we examine the effect of hosting games on local influenza transmission. The results highlight that during the flu season, for every NPB game held at its home ballpark, there is an average increase of 0.18 cases per sentinel medical institution (SMI) between that week and the following week. The effects are robust to different specifications and placebo tests. This translates to about a 0.1% increase in the number of cases during the overlap of NPB and flu seasons."
  },
  {
    "objectID": "using_r.html#wvu-empirical-workshop-22-1",
    "href": "using_r.html#wvu-empirical-workshop-22-1",
    "title": "Alexander Cardazzi",
    "section": "WVU Empirical Workshop ’22",
    "text": "WVU Empirical Workshop ’22"
  },
  {
    "objectID": "using_r.html#wvu-empirical-workshop-22-2",
    "href": "using_r.html#wvu-empirical-workshop-22-2",
    "title": "Alexander Cardazzi",
    "section": "WVU Empirical Workshop ’22",
    "text": "WVU Empirical Workshop ’22"
  },
  {
    "objectID": "using_r.html#shiny-apps",
    "href": "using_r.html#shiny-apps",
    "title": "Alexander Cardazzi",
    "section": "Shiny Apps",
    "text": "Shiny Apps\nSupply & Demand Simulation\nOLS Example\nDice Game (inspired by Quix)"
  },
  {
    "objectID": "using_r.html",
    "href": "using_r.html",
    "title": "Alexander Cardazzi",
    "section": "",
    "text": "Web Crawling & Scraping in R | Homework Solutions\nPlots & Maps"
  },
  {
    "objectID": "using_r.html#wvu-empirical-workshop-21",
    "href": "using_r.html#wvu-empirical-workshop-21",
    "title": "Alexander Cardazzi",
    "section": "WVU Empirical Workshop ’21",
    "text": "WVU Empirical Workshop ’21\nIntroduction to R | Homework Solutions\nRegressions in R | modelsummary Example\nStrings, Loops and If | Homework Solutions\nWeb Crawling & Scraping in R | Homework Solutions\nPlots & Maps"
  },
  {
    "objectID": "using_r.html#wvu-empirical-workshop-20",
    "href": "using_r.html#wvu-empirical-workshop-20",
    "title": "Alexander Cardazzi",
    "section": "WVU Empirical Workshop ’20",
    "text": "WVU Empirical Workshop ’20\nIntroduction to R | Homework Solutions\nRegressions in R | Homework Solutions\nWeb Crawling & Scraping in R"
  },
  {
    "objectID": "research.html#ad-hoc-referee",
    "href": "research.html#ad-hoc-referee",
    "title": "Alexander Cardazzi",
    "section": "Ad Hoc Referee",
    "text": "Ad Hoc Referee\nJournal of Sport Management; Contemporary Economic Policy; Heliyon; Constitutional Political Economy; Journal of Sports Economics; Sport Management Review; International Journal of Sport Finance; Economics & Human Biology; European Sport Managment Review; Eastern Economic Journal"
  },
  {
    "objectID": "using-r/ew_wvu_2021/Intro_to_R.html",
    "href": "using-r/ew_wvu_2021/Intro_to_R.html",
    "title": "Introduction to R",
    "section": "",
    "text": "All Materials Here\n\n\n\n\n\n#What is R? R is an open source (OSS), object oriented scripting language. It is mostly used by data scientists as an enviornment for statistical computing and graphics. Its source code is written in C, Fortran and R, though it stems from the mostly defunct language S. R, as a language, ranks 12th in terms of overall popularity (according to TIOBE 2019). Due to this, the answer to nearly any question you might have about R is on Google. Specifically, Stack Overflow, RStudio’s Community Website, or a Reddit Community ( r/rstats, r/Rlanguage ) should have what you are looking for. The hardest part about programming is knowing what and how to google.\nFor those of you who are curious, R is different from STATA in a few ways though ultimately minor. In my opinion, R has a steeper learning curve but it is more flexible and powerful once you are over that hump.\nWhen using R, most people use RStudio. I like to think of R as the brain and RStudio as the body. It is likely that you will never use the default R GUI (Graphical User Interface) alone.\n#Download R To download R, follow this link. R asks that you to choose a location close to you when you download. Then, you can select the R that works for your operating system.1 Then, click on Install R for the first time. Finally, Download whatever version of R is displayed. Download R before RStudio!\n#Download RStudio#### To download RStudio, go here. This should detect what OS you have and make the appropriate suggestion. However, in the event that the suggestion is incorrect, scrolling down will reveal other versions you can download.\n\nHere, we can see four distinct panels. The bottom left is the console. This is where you can view your code’s output. I also use this for “test code” I do not want in my saved file. The top left is where you write code that you want to save, which we’ll call a script. The upper right is your enviornment (along with history and connections, which we will not discuss right now). This is where you can see all the data you have loaded or created. The bottom right is where you can view plots, view files and use the help functionality. The help in R is an excellent start.\nTo use help, type in a question mark and then the function you need with into the console and hit enter. For example, type ?mean and check out what pops up. There will be information on Usage, Arguments, and Examples.\n#Data Structures####\nSome basic objects, or data structures, in R are as follows:\n\nLogical: TRUE or FALSE\nIntegers: 0L, 1L, 2L, ..., .Machine$integer.max\nDoubles: .Machine$double.xmin, ..., 0, ..., .Machine$double.xmax\nCharacters: \"Alex\", \"Econometrics\", \"Radiohead\"\nFactors: Think of these as categories. Represented numerically in R.\nVectors: A collection of anything above, but all must be of the same type.\nMatrices: A collection of vectors, all of the same type.\nLists: A collection of any of the previous objects, though does not have to be of the same type.\nData Frames: A more general form of a matrix, but a special case of a list. Arguably the most used structure in R, especially for economists running regressions.\nMissing, Empty Data: When data is missing it is displayed as NA and when data is empty it is NULL. NA values are extremely common in economic research data, and it is important to know how to handle these.\n\n#Logic Before the development of Object Oriented Programming (OOP), most notably C++, programming languages were more focused on logic instead of data. Even now with OOP, logic plays a big part in how programs are written. Here are some examples of how the “and” (&) and “or” (|) operators work in tandem with logical values. Next, we can also use inequalities like in written math, and these will be evaluated the way you might expect.\n\nA note about the semicolon used throughout this tutorial: this is just so the code is more concise. You do not need to end a line with a semicolon (like how you would in C++), but it allows you can put two lines onto one line if you separate them with a semicolon. I only ever do this if I have multiple, short lines that “go together”. Here it is purely aesthetic.\n\n\nTRUE & TRUE; TRUE & FALSE; FALSE & FALSE;\n\n[1] TRUE\n\n\n[1] FALSE\n\n\n[1] FALSE\n\nTRUE | TRUE; TRUE | FALSE; FALSE | FALSE;\n\n[1] TRUE\n\n\n[1] TRUE\n\n\n[1] FALSE\n\n5 < 3; 5 > 3; 5 <= 3; 5 >= 3; 5 == 3; 5 != 3\n\n[1] FALSE\n\n\n[1] TRUE\n\n\n[1] FALSE\n\n\n[1] TRUE\n\n\n[1] FALSE\n\n\n[1] TRUE\n\n\n\nHint: All numerical values, except 0, are coded as TRUE. I would hesitate using this unless you have binary variables that you have coded as 1/0 instead of TRUE/FALSE as it is unintuitive.\n\n\n1 & 5; 5 & 0\n\n[1] TRUE\n\n\n[1] FALSE\n\n\n\n#Arithmetic Since we aren’t interested in just logic, we can also use R as a calculator. An incredibly elaborate calculator, but a calculator nonetheless. Typical mathematical operators are what you would expect in R. +, -, *, / are addition, subtraction, multiplication and division respectively. Other useful operators are integer division (%/%) , modulo (%%), and exponents (^ or **). Integer division chops off the remainder after division and modulo returns the remainder after division.\n\n\n1 + 2; 3 - 4; 5 * 6; 7 / 8; 5%/%3; 5%%3\n\n[1] 3\n\n\n[1] -1\n\n\n[1] 30\n\n\n[1] 0.875\n\n\n[1] 1\n\n\n[1] 2\n\n\n\n#Variables / Objects Further, because we aren’t necessarily interested in just incredibly elaborate calculators, we can explore how to store data in R. Much like algebra, we can use characters, or strings of characters, to represent values. We can do this with three operators: <-, ->, =. The first two are identical besides the way they “push” data, though the first is strictly more popular than the second. The final one is different, but in a nuanced way which is beyond the scope of this tutorial.2\n\n\nx <- 1 + 1; y <- 2 + 2\nprint(x); print(y)\n\n[1] 2\n\n\n[1] 4\n\n#I am going to create a variable that is the sum of two other variables:\nx + y -> z; print(z)\n\n[1] 6\n\n#notice how changing the value of x does not change z.  This may not be trivial but it is good to note.\nx <- 100; print(z)\n\n[1] 6\n\n\n\nTypically, printing things in R is done with the print() function. However, if one is working in RStudio, just calling the actual variable will print it to the console just the same. Unless you are building a larger program that would require printing, this is not necessary to think about. Though, often times, printing is a good debugging tool when developing any script. Moreover, I encourage litering your scripts with print statements while you are learning.\n#Vectors Rarely do we work with single values. Instead, we have collections of data, or variables, which we will consider vectors. The first thing to know is c(). This stands for concatenate, and it is very important. Essentially, it coerces multiple values into a vector. The rep() function repeats the first agument passed the number of times specified by the second argument passed. The seq() function creates a sequence between the first two arguments passed with the third arguement being the level of incrimentation. Lastly, in this code block, we see how to subset vectors by indexing and by logic.\n\n\n#Note how we do not need to type every integer here by using the colon\nc(1, 2, 3, 4, 5); c(1:5)\n\n[1] 1 2 3 4 5\n\n\n[1] 1 2 3 4 5\n\nrep(1, 10); rep(c(1:5), 10)\n\n [1] 1 1 1 1 1 1 1 1 1 1\n\n\n [1] 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3\n[39] 4 5 1 2 3 4 5 1 2 3 4 5\n\nseq(1, 10, 2); seq(1, 10, .1)\n\n[1] 1 3 5 7 9\n\n\n [1]  1.0  1.1  1.2  1.3  1.4  1.5  1.6  1.7  1.8  1.9  2.0  2.1  2.2  2.3  2.4\n[16]  2.5  2.6  2.7  2.8  2.9  3.0  3.1  3.2  3.3  3.4  3.5  3.6  3.7  3.8  3.9\n[31]  4.0  4.1  4.2  4.3  4.4  4.5  4.6  4.7  4.8  4.9  5.0  5.1  5.2  5.3  5.4\n[46]  5.5  5.6  5.7  5.8  5.9  6.0  6.1  6.2  6.3  6.4  6.5  6.6  6.7  6.8  6.9\n[61]  7.0  7.1  7.2  7.3  7.4  7.5  7.6  7.7  7.8  7.9  8.0  8.1  8.2  8.3  8.4\n[76]  8.5  8.6  8.7  8.8  8.9  9.0  9.1  9.2  9.3  9.4  9.5  9.6  9.7  9.8  9.9\n[91] 10.0\n\n\n\nNow that we can make vectors, we will want to manipulate them. We can do arithmetic and logical operations to vectors like we did to scalars before.\n\n\nx <- c(1:5)\nx > 2; x + 2; x*3; x + x\n\n[1] FALSE FALSE  TRUE  TRUE  TRUE\n\n\n[1] 3 4 5 6 7\n\n\n[1]  3  6  9 12 15\n\n\n[1]  2  4  6  8 10\n\n\n\nSuppose now that we have a long vector but only want to look at, modify, remove, etc. only certain elements. We will use square brackets to do this.\n\n\n#Vector subsetting by indexing and logic\ny <- c(1:100)\ny[10:20] #by index, give me the 10th through 20th elements\n\n [1] 10 11 12 13 14 15 16 17 18 19 20\n\ny[y < 5] #by logic, give me the elements of y where y is less than 5\n\n[1] 1 2 3 4\n\ny[y %% 6 == 0] #by logic, give me the elements in y such that dividing them by 6 does not yield a remainder\n\n [1]  6 12 18 24 30 36 42 48 54 60 66 72 78 84 90 96\n\nz <- c(x, y[90:100]); z #combining two vectors, x and a subset of y.\n\n [1]   1   2   3   4   5  90  91  92  93  94  95  96  97  98  99 100\n\n\n\nAs an example, we can run some of these operations inside square brackets alone and get vectors of logical values. For example, we can use y %% 6 == 0 and get 100 TRUE or FALSE values. Sometimes, we want to know the indices where these are TRUE and we can use the which() function. This is an important function.\nYou might be able to see how this can become complicated. For example, suppose we had 3 vectors: GDP, Country, and Continent, and we only want the GDP for countries in Europe. It may look something like this:\n\n\nGDP <- c(47, 23, 61, 29, 80, 48, 92, 42)\nCountry <- c(\"USA\", \"Italy\", \"Egypt\", \"Mexico\", \"Japan\", \"UK\", \"Germany\", \"Brazil\")\nContinent <- c(\"NA\", \"EU\", \"AF\", \"NA\", \"AS\", \"EU\", \"EU\", \"SA\")\nprint(GDP[Continent == \"EU\"])\n\n[1] 23 48 92\n\n\n\nI like to think of it like: “Give me GDP where/given Continent equals EU”. Talk to yourself when you program! If you wanted continent to be equal to europe OR north america, this can be done like: Continent == \"EU\" | Continent == \"NA\" or Continent %in% c(\"EU\", \"NA\").\nOverall, vectors are of extreme interest to us. I want to take a moment to show some helpful, prebuilt statistical functions in R. For example, we can use mean(), sum(), sd(), var(), summary(), t.test(), etc. There are also normal distribution functions rnorm(), qnorm(), dnorm(), and pnorm().\n\n\nN <- 100\nset.seed(789) #this is so we can replicate randomness\nx <- rnorm(N, mean = 1, sd = 4)\ny <- rnorm(N, mean = 0, sd = 4)\nsummary(x)\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n-11.3505  -1.8126   0.3370   0.8459   3.6653  10.3941 \n\nsummary(y)\n\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n-11.91203  -2.92133  -0.07153  -0.28624   2.27216  12.20789 \n\nt.test(x,y)\n\n\n    Welch Two Sample t-test\n\ndata:  x and y\nt = 1.9832, df = 197.21, p-value = 0.04873\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n 0.006345714 2.257987623\nsample estimates:\n mean of x  mean of y \n 0.8459310 -0.2862356 \n\n\n\n#Matrices In the following section, we will learn a little bit about how matrix operations work in R. Typically, most data is held in data frames, but first it is valuable to look into matrices. We will begin with two vectors of 5 randomly generated numbers.\n\n\n#rnorm generates n numbers from a normal distribution ... use ?rnorm for help\n#rbind - combine by row, or stack on top\n#cbind - combine by column, or squish next to each other\nset.seed(456) #set.seed allows us to replicate randomness\nx1 <- rnorm(5); x1\n\n[1] -1.3435214  0.6217756  0.8008747 -1.3888924 -0.7143569\n\nx2 <- rnorm(5); x2\n\n[1] -0.3240611  0.6906430  0.2505479  1.0073523  0.5732347\n\nrbindEx <- rbind(x1, x2); rbindEx\n\n         [,1]      [,2]      [,3]      [,4]       [,5]\nx1 -1.3435214 0.6217756 0.8008747 -1.388892 -0.7143569\nx2 -0.3240611 0.6906430 0.2505479  1.007352  0.5732347\n\ncbindEx <- cbind(x1, x2); cbindEx\n\n             x1         x2\n[1,] -1.3435214 -0.3240611\n[2,]  0.6217756  0.6906430\n[3,]  0.8008747  0.2505479\n[4,] -1.3888924  1.0073523\n[5,] -0.7143569  0.5732347\n\nrbindEx * 10\n\n         [,1]     [,2]     [,3]      [,4]      [,5]\nx1 -13.435214 6.217756 8.008747 -13.88892 -7.143569\nx2  -3.240611 6.906430 2.505479  10.07352  5.732347\n\n\n\nNow, we have created two matrices with our data. You can transpose them (t()), multiply them (%*%) or invert them (solve()). Instead of going too far into matrices, we’re going to move onto lists.\n#Lists Lists are a super flexible way to collect things. Here is an example of a list:\n\nx <- list(c(\"a\", \"b\", \"c\"),\n          \"d\",\n          list(1,\n               c(2, 3),\n               \"alex\")\n          )\n\nHere, my list x has three elements. There is a character vector, a character element and then another list all together. To access the elements of a list, we need a double square bracket like x[[1]] which will yield the entire vector c(\"a\", \"b\", \"c\"). x[[1]][1] will yield \"a\".\nWe can also name elements in a list and use these to access certain elements.3\n\n\natm <- list(\n  \n  balance = c(5, 2, 3),\n  name = list(\n    first = c(\"alex\", \"bryan\", \"brad\"),\n    last = c(\"cardazzi\", \"mccannon\", \"humphreys\")\n    ),\n  favoriteSport = c(\"basketball\", \"nascar\", \"wvu football\")\n  )\n\natm$favoriteSport\n\n[1] \"basketball\"   \"nascar\"       \"wvu football\"\n\natm$name$first\n\n[1] \"alex\"  \"bryan\" \"brad\" \n\natm[[2]][[1]]\n\n[1] \"alex\"  \"bryan\" \"brad\" \n\n\n\nYou might imagine taking this list and converting it into something that looks like a table. This is where data frames come in.\n#Data Frames\nData frames are just a special case of a list. In a data frame, you should think of a matrix, or an excel sheet, where each column has a name. Typically, we read in data from CSV, Excel, JSON files, etc. In this tutorial, I will strictly use CSV files. Here is how to save your excel files as CSVs if you are unsure how. I am going to use this file called knicks.csv and it can be found here. We will talk more about reading data into R later.\n\n\nknicks <- read.csv(\"C:/Users/alexc/Desktop/Empirical Workshop/data/knicks.csv\", stringsAsFactors = FALSE)\ndim(knicks) #returns the rows and columns of a data frame\n\n[1] 23 10\n\nhead(knicks) #head will display the first 6 rows just so you can take a peak at the data\n\n  No.                    Player Pos     Ht Ht_inches  Wt       Birth.Date  X\n1   0   Kadeem Allen\\\\allenka01  SG  1-Jun        73 200  January 15 1993 us\n2  31      Ron Baker\\\\bakerro01  SG  4-Jun        76 220    March 30 1993 us\n3  23     Trey Burke\\\\burketr01  PG Jun-00        73 175 November 12 1992 us\n4  21 Damyean Dotson\\\\dotsoda01  SG  5-Jun        77 202       May 6 1994 us\n5  13 Henry Ellenson\\\\ellenhe01  PF 10-Jun        82 240  January 13 1997 us\n6   3  Billy Garrett\\\\garrebi01  SG  6-Jun        78 213  October 16 1994 us\n  Exp        College\n1   1        Arizona\n2   2  Wichita State\n3   5       Michigan\n4   1 Oregon Houston\n5   2      Marquette\n6   R         DePaul\n\n\n\nWe used square brackets when subsetting vector, and we will do the same here. However, there are two dimensions (rows and columns) as opposed to the one dimensional vectors before. To obtain just the weight column, we can do knicks$Wt. To get the weight of players who did not attend college, we can do knicks$Wt[knicks$College == \"\"]. Note the column knicks$Exp. There are numbers but all of them have quotes around them. This is because some players have an “R” in their row, which means the player is a rookie and this is their first year in the NBA. In other words, if there is a vector of all numeric elements and one character elements, the whole vector will be converted to a character. You can force a vector to be numeric by as.numeric(). Here, the “R” will be removed for an NA, since the program can turn \"9\" into 9, but gets confused trying to convert \"R\" into a number.\n\nHomework 1\nFor this homework, use knicks.csv. Try your best not to “hard code” anything. For example, there are 23 Knicks players, but try not to type the number 23 in your code. This is to keep the script as flexible as possible, incase you had to repeat this code for a completely different roster list. Use View(knicks) to look at the dataset.\n\nFind the average weight of the Knicks roster.\nFind the range of the weights.\nFind the standard deviation of the weights.\nCalculate the standard deviation of the weights without using either sd() or var().\nTest if there is a significant difference in the weights of point guards & shooting guards relative to the rest of the players. Hint: point guard = PG, shooting guard = SG in the Pos. column.\nTest if guards (PG, SG) tend to have lower jersey numbers than other positions, but remove the centers (C).\nFind the average experience of the players.\nGenerate a vector of the names of the 5 least heavy players\nGenerate a vector of the names of the 5 heaviest players\nDrop the rows where players did not go to college or they are foreign. Save this as knicks2\nDrop the College column from the dataset.\nCheck if there is a significant correlation between jersey number and experience.\nMake a correlation matrix for weight, jersey number and experience.\n\n\n\n\n\n\nFootnotes\n\n\nThese tutorials assume you have Windows. The differences in Mac, Windows, Linux mostly have to do with reading in data since file paths are different. For example, in Windows, you might see a file path like C:/Users/alexc/Documents whereas on a Mac it might look like ~/Documents. We will cover this later.↩︎\nIf you are interested, a link↩︎\nThis is a good example of when to use <- vs =. Take “balance” as an example. The equal sign will make “balance” exist inside the list, but not in our Global Enviornment window pane. Instead, I could have written balance <- c(5, 2, 3) but this would have saved balance inside the atm list and in our Global Enviornment. Usually, this is not desired, but there may be occasions.↩︎"
  },
  {
    "objectID": "media.html",
    "href": "media.html",
    "title": "In the Media",
    "section": "",
    "text": "As Pro Sports Ponder Reopening, Flu Study Suggests Danger of COVID Spread\nU.S. cities with pro sports see more flu deaths"
  },
  {
    "objectID": "media.html#nber-economics-of-transportation-in-the-21st-century",
    "href": "media.html#nber-economics-of-transportation-in-the-21st-century",
    "title": "In the Media",
    "section": "NBER Economics of Transportation in the 21st Century",
    "text": "NBER Economics of Transportation in the 21st Century\nWVU economists fueled to pave the way for enhanced traffic safety, road quality\nThe U.S. Needs to Fix Existing Roads, Not Build New Ones"
  },
  {
    "objectID": "econ311.html",
    "href": "econ311.html",
    "title": "Econ 311",
    "section": "",
    "text": "Module 0: Course Introduction\n\n\nSyllabus (file)\nModule 0.1: Introduction (slides | notes)\n\n\n\n\nModule 1: Foundations of Data\n\n\nModule 1.1: Types of Data (slides | notes)\nModule 1.2: Downloading R (slides | notes)\nModule 1.3: Using R (slides | notes)\nHomework 1 (file)\n\n\n\n\nModule 2: One Y Variable\n\n\nModule 2.1: Central Tendency (slides | notes)\nModule 2.2: Dispersion (slides | notes)\nModule 2.3: Distributions (slides | notes)\nModule 2.4: Functions and Loops (slides | notes)\nModule 2.5: Hypothesis Testing (slides | notes)\nHomework 2 (file)\n\n\n\n\nModule 3: One X Variable\n\n\nModule 3.1: Correlation (slides | notes)\nModule 3.2: Linear Regression (slides | notes)\nModule 3.3: OLS in R (slides | notes)\nHomework 3 (file)\n\n\n\n\nModule 4: Many X Variables\n\n\nModule 4.1: Multiple Regression (slides | notes)\nModule 4.2: Binary Variables and Interactions (slides | notes)\nHomework 4 (file)\n\n\n\n\nModule 5: Special Topics in R\n\n\nModule 5.1: Text as Data (slides | notes)\nModule 5.2: Dates as Data (slides | notes)\nModule 5.3: The Internet as Data (slides | notes)\nHomework 5 (file)\n\n\n\n\nResources, Data, Misc.\n\n\nFAQ / Common Mistakes (file)\nCourse Data (file)"
  },
  {
    "objectID": "software.html",
    "href": "software.html",
    "title": "Alexander Cardazzi",
    "section": "",
    "text": "Local Projections Difference in Differences\n\n\n\n\n\nR implementation of Dube et al. (2023).\nInstall by devtools::install_github(\"alexCardazzi/lpdid\"), and load by library(lpdid). The package vignette can be found here\nVersion 0.1.0, written with Zach Porreca."
  }
]